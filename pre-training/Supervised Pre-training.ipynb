{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfWd0i24RYx3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfd44HO8KuAQ"
      },
      "source": [
        "%%capture\n",
        "!wget https://www.dropbox.com/s/n7rt3s9jjznnd2d/concept_relationship_prime_prime.csv?dl=0\n",
        "!pip install transformers\n",
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqQWyvS6LrzA"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1QiU5KULut6"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iVStzJALz_e"
      },
      "source": [
        "CONFIG={\n",
        "    'DATA_PATH':'.//content/concept_relationship_prime_prime.csv?dl=0', # PATH TO UNZIP DATASET\n",
        "    'concept1':'concept_id_1',\n",
        "    'concept2':'concept_id_2',\n",
        "    'labels':'relationship_id',\n",
        "    'SEED':13,\n",
        "    'MAX_LEN':256,\n",
        "    'MODEL_NAME_OR_PATH':'dmis-lab/biobert-v1.1',\n",
        "    'LEARNING_RATE':2e-5,\n",
        "    'ADAM_EPSILON':1e-8,\n",
        "    'WEIGHT_DECAY':0.0,\n",
        "    'NUM_CLASSES':100,\n",
        "    'TRAIN_BS':32,\n",
        "    'VAL_BS':32,\n",
        "    'WARMUP_STEPS':0,\n",
        "    'MAX_EPOCHS':1,\n",
        "    'CHECKPOINT_DIR':'./checkpoints',\n",
        "    'NUM_WORKERS':2,\n",
        "    'PRECISION':16,\n",
        "    'MODEL_SAVE_NAME':'biobert_v1'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWXCbG8lL2u1"
      },
      "source": [
        "_=pl.seed_everything(CONFIG['SEED'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fURJBn0TL5RB"
      },
      "source": [
        "class ConceptLearningDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, max_len: int, tokenizer, concept1, concept2, labels):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.concept1 = concept1\n",
        "        self.concept2 = concept2\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.concept1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        concept_1 = self.concept1[idx]\n",
        "        concept_2 = self.concept2[idx]\n",
        "        encoded_input = self.tokenizer.encode_plus(\n",
        "            text=concept_1,\n",
        "            text_pair=concept_2,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'labels': torch.tensor(self.labels[idx]),\n",
        "            'input_ids': encoded_input['input_ids'].view(-1),\n",
        "            'attention_mask': encoded_input['attention_mask'].view(-1),\n",
        "            'token_type_ids': encoded_input['token_type_ids'].view(-1),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2OpA7iUMuKe"
      },
      "source": [
        "class ConceptLearningDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def prepare_data(self):\n",
        "      # self.data_df=pd.read_csv(CONFIG['DATA_PATH'])\n",
        "      self.data_df=pd.read_csv('./concept_relationship_prime_prime.csv?dl=0',nrows=10_000)\n",
        "      self.label_encoder=LabelEncoder()\n",
        "      self.data_df[CONFIG['labels']]=self.label_encoder.fit_transform(self.data_df[CONFIG['labels']])\n",
        "      self.tokenizer=transformers.AutoTokenizer.from_pretrained(CONFIG['MODEL_NAME_OR_PATH'])\n",
        "\n",
        "    def setup(self, stage):\n",
        "      if stage=='fit':\n",
        "        self.train_df,self.val_df=train_test_split(self.data_df,random_state=CONFIG['SEED'],test_size=0.1)\n",
        "\n",
        "    def get_dataset(self,df):\n",
        "      dataset = ConceptLearningDataset(max_len=CONFIG['MAX_LEN'],\n",
        "                               tokenizer=self.tokenizer,\n",
        "                               concept1=df[CONFIG['concept1']].values,\n",
        "                               concept2=df[CONFIG['concept2']].values,\n",
        "                               labels=df[CONFIG['labels']].values)\n",
        "      return dataset\n",
        "\n",
        "    def train_dataloader(self):\n",
        "      train_dataset=self.get_dataset(self.train_df)\n",
        "      train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                                     batch_size=CONFIG['TRAIN_BS'], \n",
        "                                                     shuffle=True, \n",
        "                                                     num_workers=CONFIG['NUM_WORKERS'])\n",
        "      \n",
        "      return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "      val_dataset=self.get_dataset(self.val_df)\n",
        "      val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                                     batch_size=CONFIG['VAL_BS'], \n",
        "                                                     shuffle=False, \n",
        "                                                     num_workers=CONFIG['NUM_WORKERS'])\n",
        "      \n",
        "      return val_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnEY5pQTN7tP"
      },
      "source": [
        "class ConceptLearningModel(pl.LightningModule):\n",
        "\n",
        "  def __init__(self,model_name_or_path:str,\n",
        "               num_labels:int,\n",
        "               learning_rate:float,\n",
        "               adam_epsilon:float,\n",
        "               weight_decay:float,\n",
        "               max_len:int,\n",
        "               warmup_steps:int,\n",
        "               gpus:int,max_epochs:int,accumulate_grad_batches:int):\n",
        "    super().__init__()\n",
        "    self.model_name_or_path=model_name_or_path\n",
        "    self.num_labels=num_labels\n",
        "    \n",
        "    self.save_hyperparameters('learning_rate','adam_epsilon','weight_decay','max_len','gpus','accumulate_grad_batches','max_epochs','warmup_steps') \n",
        "\n",
        "    self.config = transformers.AutoConfig.from_pretrained(model_name_or_path, num_labels=self.num_labels)\n",
        "    self.model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
        "    metrics = torchmetrics.MetricCollection([\n",
        "        torchmetrics.Accuracy(),\n",
        "        torchmetrics.F1(num_classes=CONFIG['NUM_CLASSES'])\n",
        "      ]\n",
        "    )\n",
        "    self.train_metrics=metrics.clone()\n",
        "    self.val_metrics=metrics.clone()\n",
        "\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    return self.model(**inputs)\n",
        "  \n",
        "  def training_step(self,batch,batch_idx):\n",
        "    loss,logits=self(batch)[:2]\n",
        "    predictions=torch.argmax(logits,dim=1)\n",
        "    self.train_metrics(predictions,batch['labels'])\n",
        "    self.log_dict({'train_accuracy':self.train_metrics['Accuracy'],'train_f1':self.train_metrics['F1']}, on_step=False, on_epoch=True)\n",
        "    return {\n",
        "        'loss':loss,\n",
        "        'predictions':predictions,\n",
        "        'labels':batch['labels']\n",
        "    }\n",
        "  \n",
        "  def validation_step(self,batch,batch_idx):\n",
        "    loss,logits=self(batch)[:2]\n",
        "    predictions=torch.argmax(logits,dim=1)\n",
        "    self.val_metrics(predictions,batch['labels'])\n",
        "    self.log_dict({'val_accuracy':self.val_metrics['Accuracy'],'val_f1':self.val_metrics['F1']}, on_step=False, on_epoch=True)\n",
        "    return {\n",
        "        'loss':loss,\n",
        "        'predictions':predictions,\n",
        "        'labels':batch['labels']\n",
        "    }\n",
        "\n",
        "  def validation_epoch_end(self,outputs):\n",
        "    loss=torch.tensor([x['loss'] for x in outputs])\n",
        "    loss = loss.mean()\n",
        "    self.log('val_loss', loss, prog_bar=True,on_step=False, on_epoch=True )\n",
        "  \n",
        "  def training_epoch_end(self,outputs):\n",
        "    loss=torch.tensor([x['loss'] for x in outputs])\n",
        "    loss = loss.mean()\n",
        "    self.log('train_loss', loss, prog_bar=True,on_step=False, on_epoch=True )\n",
        "  \n",
        "  def setup(self, stage):\n",
        "    if stage == 'fit':\n",
        "      train_loader = self.train_dataloader()\n",
        "      self.total_steps = (\n",
        "          (len(train_loader.dataset) // (train_loader.batch_size * max(1, self.hparams.gpus)))\n",
        "          // self.hparams.accumulate_grad_batches * float(self.hparams.max_epochs)\n",
        "      )\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    model = self.model\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\",\"LayerNorm.bias\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "          {\n",
        "              \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "              \"weight_decay\": self.hparams.weight_decay,\n",
        "          },\n",
        "          {\n",
        "              \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "              \"weight_decay\": 0.0,\n",
        "          },\n",
        "    ]\n",
        "    optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n",
        "    )\n",
        "    scheduler = {\n",
        "        'scheduler': scheduler,\n",
        "        'interval': 'step',\n",
        "        'frequency': 1\n",
        "    }\n",
        "    return [optimizer] ,[scheduler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqExoxvvPYyz"
      },
      "source": [
        "model_save_checkpoint = pl.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath=CONFIG['CHECKPOINT_DIR'],\n",
        "    filename=f\"{CONFIG['MODEL_SAVE_NAME']}\"+'-{epoch:02d}-{val_loss:.2f}',\n",
        "    save_top_k=1,\n",
        "    mode='min',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glepz8mwPdpT"
      },
      "source": [
        "trainer = pl.Trainer(gpus=torch.cuda.device_count(),\n",
        "                     max_epochs=CONFIG['MAX_EPOCHS'],\n",
        "                     callbacks=[model_save_checkpoint],\n",
        "                    #  precision=CONFIG['PRECISION'],\n",
        "                     num_sanity_val_steps=0\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rif2MeW2PrYq"
      },
      "source": [
        "model=ConceptLearningModel(\n",
        "    model_name_or_path=CONFIG['MODEL_NAME_OR_PATH'],\n",
        "    num_labels=CONFIG['NUM_CLASSES'],\n",
        "    learning_rate=CONFIG['LEARNING_RATE'],\n",
        "    adam_epsilon=CONFIG['ADAM_EPSILON'],\n",
        "    weight_decay=CONFIG['WEIGHT_DECAY'],\n",
        "    max_len=CONFIG['MAX_LEN'],\n",
        "    warmup_steps=CONFIG['WARMUP_STEPS'],\n",
        "    max_epochs=trainer.max_epochs,\n",
        "    gpus=trainer.gpus,\n",
        "    accumulate_grad_batches=trainer.accumulate_grad_batches,\n",
        ")\n",
        "\n",
        "mnli_dm=ConceptLearningDataModule()\n",
        "trainer.fit(model,mnli_dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu8yKnE-P4I4"
      },
      "source": [
        "%tensorboard --logdir ./lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}