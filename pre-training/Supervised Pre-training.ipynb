{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pfWd0i24RYx3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  3 02:48:36 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   59C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gqQWyvS6LrzA"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c1QiU5KULut6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5iVStzJALz_e"
   },
   "outputs": [],
   "source": [
    "CONFIG={\n",
    "    'DATA_PATH':'../../datasets/concept_relationship_prime.csv', # PATH TO UNZIP DATASET\n",
    "    'concept1':'concept_id_1',\n",
    "    'concept2':'concept_id_2',\n",
    "    'labels':'relationship_id',\n",
    "    'SEED':13,\n",
    "    'MAX_LEN':256,\n",
    "    'MODEL_NAME_OR_PATH':'dmis-lab/biobert-v1.1',\n",
    "    'LEARNING_RATE':2e-5,\n",
    "    'ADAM_EPSILON':1e-8,\n",
    "    'WEIGHT_DECAY':0.0,\n",
    "    'NUM_CLASSES':30,\n",
    "    'TRAIN_BS':32,\n",
    "    'VAL_BS':32,\n",
    "    'WARMUP_STEPS':0,\n",
    "    'MAX_EPOCHS':5,\n",
    "    'CHECKPOINT_DIR':'./checkpoints',\n",
    "    'NUM_WORKERS':8,\n",
    "    'PRECISION':16,\n",
    "    'MODEL_SAVE_NAME':'biobert_v1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EWXCbG8lL2u1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    }
   ],
   "source": [
    "_=pl.seed_everything(CONFIG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fURJBn0TL5RB"
   },
   "outputs": [],
   "source": [
    "class ConceptLearningDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, max_len: int, tokenizer, concept1, concept2, labels):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concept1 = concept1\n",
    "        self.concept2 = concept2\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concept1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concept_1 = self.concept1[idx]\n",
    "        concept_2 = self.concept2[idx]\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            text=concept_1,\n",
    "            text_pair=concept_2,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'labels': torch.tensor(self.labels[idx]),\n",
    "            'input_ids': encoded_input['input_ids'].view(-1),\n",
    "            'attention_mask': encoded_input['attention_mask'].view(-1),\n",
    "            'token_type_ids': encoded_input['token_type_ids'].view(-1),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e2OpA7iUMuKe"
   },
   "outputs": [],
   "source": [
    "class ConceptLearningDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def prepare_data(self):\n",
    "      self.data_df=pd.read_csv(CONFIG['DATA_PATH'],nrows=40_000)\n",
    "      print(\"Types of Relationship \",self.data_df['relationship_id'].unique().shape)\n",
    "      self.label_encoder=LabelEncoder()\n",
    "      self.data_df[CONFIG['labels']]=self.label_encoder.fit_transform(self.data_df[CONFIG['labels']])\n",
    "      self.tokenizer=transformers.AutoTokenizer.from_pretrained(CONFIG['MODEL_NAME_OR_PATH'])\n",
    "\n",
    "    def setup(self, stage):\n",
    "      if stage=='fit':\n",
    "        self.train_df,self.val_df=train_test_split(self.data_df,random_state=CONFIG['SEED'],test_size=0.1)\n",
    "\n",
    "    def get_dataset(self,df):\n",
    "      dataset = ConceptLearningDataset(max_len=CONFIG['MAX_LEN'],\n",
    "                               tokenizer=self.tokenizer,\n",
    "                               concept1=df[CONFIG['concept1']].values,\n",
    "                               concept2=df[CONFIG['concept2']].values,\n",
    "                               labels=df[CONFIG['labels']].values)\n",
    "      return dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "      train_dataset=self.get_dataset(self.train_df)\n",
    "      train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                     batch_size=CONFIG['TRAIN_BS'], \n",
    "                                                     shuffle=True, \n",
    "                                                     num_workers=CONFIG['NUM_WORKERS'])\n",
    "      \n",
    "      return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "      val_dataset=self.get_dataset(self.val_df)\n",
    "      val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                                     batch_size=CONFIG['VAL_BS'], \n",
    "                                                     shuffle=False, \n",
    "                                                     num_workers=CONFIG['NUM_WORKERS'])\n",
    "      \n",
    "      return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DnEY5pQTN7tP"
   },
   "outputs": [],
   "source": [
    "class ConceptLearningModel(pl.LightningModule):\n",
    "\n",
    "  def __init__(self,model_name_or_path:str,\n",
    "               num_labels:int,\n",
    "               learning_rate:float,\n",
    "               adam_epsilon:float,\n",
    "               weight_decay:float,\n",
    "               max_len:int,\n",
    "               warmup_steps:int,\n",
    "               gpus:int,max_epochs:int,accumulate_grad_batches:int):\n",
    "    super().__init__()\n",
    "    self.model_name_or_path=model_name_or_path\n",
    "    self.num_labels=num_labels\n",
    "    \n",
    "    self.save_hyperparameters('learning_rate','adam_epsilon','weight_decay','max_len','gpus','accumulate_grad_batches','max_epochs','warmup_steps') \n",
    "\n",
    "    self.config = transformers.AutoConfig.from_pretrained(model_name_or_path, num_labels=self.num_labels)\n",
    "    self.model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
    "    metrics = torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(),\n",
    "        torchmetrics.F1(num_classes=CONFIG['NUM_CLASSES'])\n",
    "      ]\n",
    "    )\n",
    "    self.train_metrics=metrics.clone()\n",
    "    self.val_metrics=metrics.clone()\n",
    "\n",
    "\n",
    "  def forward(self,inputs):\n",
    "    return self.model(**inputs)\n",
    "  \n",
    "  def training_step(self,batch,batch_idx):\n",
    "    loss,logits=self(batch)[:2]\n",
    "    predictions=torch.argmax(logits,dim=1)\n",
    "    self.train_metrics(predictions,batch['labels'])\n",
    "    self.log_dict({'train_accuracy':self.train_metrics['Accuracy'],'train_f1':self.train_metrics['F1']}, on_step=False, on_epoch=True)\n",
    "    return {\n",
    "        'loss':loss,\n",
    "        'predictions':predictions,\n",
    "        'labels':batch['labels']\n",
    "    }\n",
    "  \n",
    "  def validation_step(self,batch,batch_idx):\n",
    "    loss,logits=self(batch)[:2]\n",
    "    predictions=torch.argmax(logits,dim=1)\n",
    "    self.val_metrics(predictions,batch['labels'])\n",
    "    self.log_dict({'val_accuracy':self.val_metrics['Accuracy'],'val_f1':self.val_metrics['F1']}, on_step=False, on_epoch=True)\n",
    "    return {\n",
    "        'loss':loss,\n",
    "        'predictions':predictions,\n",
    "        'labels':batch['labels']\n",
    "    }\n",
    "\n",
    "  def validation_epoch_end(self,outputs):\n",
    "    loss=torch.tensor([x['loss'] for x in outputs])\n",
    "    loss = loss.mean()\n",
    "    self.log('val_loss', loss, prog_bar=True,on_step=False, on_epoch=True )\n",
    "  \n",
    "  def training_epoch_end(self,outputs):\n",
    "    loss=torch.tensor([x['loss'] for x in outputs])\n",
    "    loss = loss.mean()\n",
    "    self.log('train_loss', loss, prog_bar=True,on_step=False, on_epoch=True )\n",
    "  \n",
    "  def setup(self, stage):\n",
    "    if stage == 'fit':\n",
    "      train_loader = self.train_dataloader()\n",
    "      self.total_steps = (\n",
    "          (len(train_loader.dataset) // (train_loader.batch_size * max(1, self.hparams.gpus)))\n",
    "          // self.hparams.accumulate_grad_batches * float(self.hparams.max_epochs)\n",
    "      )\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    model = self.model\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\",\"LayerNorm.bias\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "          {\n",
    "              \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "              \"weight_decay\": self.hparams.weight_decay,\n",
    "          },\n",
    "          {\n",
    "              \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "              \"weight_decay\": 0.0,\n",
    "          },\n",
    "    ]\n",
    "    optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n",
    "    )\n",
    "    scheduler = {\n",
    "        'scheduler': scheduler,\n",
    "        'interval': 'step',\n",
    "        'frequency': 1\n",
    "    }\n",
    "    return [optimizer] ,[scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qqExoxvvPYyz"
   },
   "outputs": [],
   "source": [
    "model_save_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=CONFIG['CHECKPOINT_DIR'],\n",
    "    filename=f\"{CONFIG['MODEL_SAVE_NAME']}\"+'-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "glepz8mwPdpT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=torch.cuda.device_count(),\n",
    "                     max_epochs=CONFIG['MAX_EPOCHS'],\n",
    "                     callbacks=[model_save_checkpoint],\n",
    "                    #  precision=CONFIG['PRECISION'],\n",
    "                     num_sanity_val_steps=0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rif2MeW2PrYq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Relationship  (30,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                          | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model         | BertForSequenceClassification | 108 M \n",
      "1 | train_metrics | MetricCollection              | 0     \n",
      "2 | val_metrics   | MetricCollection              | 0     \n",
      "----------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.333   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25159860e7594ca38e9ff4c746368eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/va2134/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/va2134/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConceptLearningDataModule' object has no attribute 'has_teardown_None'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e8e0deae8366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmnli_dm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConceptLearningDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnli_dm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    867\u001b[0m         )\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m# teardown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_teardown_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTERRUPTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_teardown_hook\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mcalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'has_teardown_{state}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcalled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConceptLearningDataModule' object has no attribute 'has_teardown_None'"
     ]
    }
   ],
   "source": [
    "model=ConceptLearningModel(\n",
    "    model_name_or_path=CONFIG['MODEL_NAME_OR_PATH'],\n",
    "    num_labels=CONFIG['NUM_CLASSES'],\n",
    "    learning_rate=CONFIG['LEARNING_RATE'],\n",
    "    adam_epsilon=CONFIG['ADAM_EPSILON'],\n",
    "    weight_decay=CONFIG['WEIGHT_DECAY'],\n",
    "    max_len=CONFIG['MAX_LEN'],\n",
    "    warmup_steps=CONFIG['WARMUP_STEPS'],\n",
    "    max_epochs=trainer.max_epochs,\n",
    "    gpus=trainer.gpus,\n",
    "    accumulate_grad_batches=trainer.accumulate_grad_batches,\n",
    ")\n",
    "\n",
    "mnli_dm=ConceptLearningDataModule()\n",
    "trainer.fit(model,mnli_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': tensor(0.7863, device='cuda:0'),\n",
       " 'train_f1': tensor(0.7863, device='cuda:0'),\n",
       " 'epoch': tensor(1.),\n",
       " 'train_loss': tensor(0.5545, device='cuda:0'),\n",
       " 'val_accuracy': tensor(0.7860),\n",
       " 'val_f1': tensor(0.7860),\n",
       " 'val_loss': tensor(0.5301)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Bu8yKnE-P4I4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4a6f188a424e617b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4a6f188a424e617b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
